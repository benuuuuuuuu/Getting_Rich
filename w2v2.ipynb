{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "import datetime\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "import codecs\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "def get_sql_data(sql):\n",
    "    conn = pymysql.connect(host='localhost', user='root', password='1qaz!QAZ', database='twitter')\n",
    "    sqlcmd = sql\n",
    "    data = pd.read_sql(sqlcmd, conn)\n",
    "    conn.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def ab(df):\n",
    "    return ' '.join(df.values)\n",
    "\n",
    "\n",
    "def all_posts_oneday(data):\n",
    "    posts = data.groupby(['ticker', 'created_time'])['text'].apply(ab).reset_index()\n",
    "    posts['retweets'] = data.groupby(['ticker', 'created_time'])['retweets'].apply(sum).reset_index().iloc[:, -1]\n",
    "    posts['replies'] = data.groupby(['ticker', 'created_time'])['replies'].apply(sum).reset_index().iloc[:, -1]\n",
    "    posts['likes'] = data.groupby(['ticker', 'created_time'])['likes'].apply(sum).reset_index().iloc[:, -1]\n",
    "    posts['is_retweet'] = data.groupby(['ticker', 'created_time'])['is_retweet'].apply(sum).reset_index().iloc[:, -1]\n",
    "\n",
    "    posts['created_time'] = pd.to_datetime(posts['created_time'], format=\"%Y-%m-%d\")\n",
    "    posts['rank'] = posts.groupby(posts['ticker'])['created_time'].rank()\n",
    "    return posts\n",
    "\n",
    "\n",
    "def merge_days_posts(days, posts):\n",
    "    posts_tmp = posts.copy().loc[:, ['ticker', 'created_time', 'rank', 'text']]\n",
    "    for day in range(days):\n",
    "        n = day + 1\n",
    "        col = 'rank' + str(1)\n",
    "        post_col = 'rank' + str(n) + 'posts'\n",
    "        posts_tmp[col] = posts.apply(lambda x: int(x[3] + n), axis=1)\n",
    "        posts_tmp = pd.merge(posts_tmp, posts.loc[:, ['ticker', 'rank', 'text']], how='left', left_on=['ticker', col],\n",
    "                             right_on=['ticker', 'rank'], suffixes=['', '_r'])\n",
    "        posts_tmp = posts_tmp.drop(['rank_r', col], axis=1)\n",
    "        posts_tmp = posts_tmp.rename(columns={'text_r': 'text' + str(n)})\n",
    "        posts_tmp = posts_tmp.dropna()\n",
    "        posts_tmp['days_posts'] = posts_tmp.apply(lambda x: ' '.join(x[4:]), axis=1)\n",
    "        final = posts_tmp.loc[:, ['ticker', 'created_time', 'days_posts']]\n",
    "    return final\n",
    "\n",
    "\n",
    "def tokenize(posts):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    posts[\"tokens\"] = posts[\"days_posts\"].apply(tokenizer.tokenize)\n",
    "    posts = posts.loc[:, ['ticker', 'created_time', 'tokens']].reset_index().iloc[:, 1:]\n",
    "    return posts\n",
    "\n",
    "\n",
    "def get_close_price_nday(day):\n",
    "    closeeps = pd.read_csv('closeeps.csv', low_memory=False).iloc[:, 2:]\n",
    "    print(closeeps)\n",
    "    closeeps = closeeps.drop(['cshoc', 'cshtrd', 'eps'], axis=1)\n",
    "    closeeps.columns = ['created_time', 'ticker', 'cp']\n",
    "    closeeps['created_time'] = pd.to_datetime(closeeps['created_time'], format=\"%Y%m%d\")\n",
    "    closeeps['ticker'] = '$' + closeeps['ticker']\n",
    "    closeeps['rank'] = closeeps.groupby(closeeps['ticker'])['created_time'].rank()\n",
    "    closeeps['rank2'] = closeeps.apply(lambda x: x['rank'] + day, axis=1)\n",
    "    close = pd.merge(closeeps, closeeps, left_on=['ticker', 'rank2'], right_on=['ticker', 'rank'], suffixes=['', '_r'])\n",
    "    close = close.loc[:, ['ticker', 'created_time', 'cp', 'cp_r']]\n",
    "    close = close.rename(columns={'cp_r': 'cp' + str(day)})\n",
    "    return close\n",
    "\n",
    "\n",
    "def merge_posts_price(token_data, closeprice, target_rr):\n",
    "    data = pd.merge(token_data, closeprice, on=['ticker', 'created_time'], suffixes=['', ''])\n",
    "    data['rr'] = data.apply(lambda x: (x[-1] - x[-2]) / (x[-2]), axis=1)\n",
    "    data['tf'] = data.apply(lambda x: x['rr'] >= target_rr, axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def word_to_vec(data):\n",
    "    def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "        if len(tokens_list) < 1:\n",
    "            return np.zeros(k)\n",
    "        if generate_missing:\n",
    "            vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "        else:\n",
    "            vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "\n",
    "        length = len(vectorized)\n",
    "        # print(length)\n",
    "        summed = np.sum(vectorized, axis=0)\n",
    "        averaged = np.divide(summed, length)\n",
    "        return averaged\n",
    "\n",
    "    def get_word2vec_embeddings(vectors, df, generate_missing=False):\n",
    "        embeddings = df['tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n",
    "        return embeddings\n",
    "\n",
    "    embeddings = get_word2vec_embeddings(word2vec, data).to_list()\n",
    "    embeddings = np.array(embeddings)\n",
    "    embeddings = pd.DataFrame(embeddings)\n",
    "    #     pca = PCA(n_components=n_components, random_state=0)\n",
    "    #     pcs = pd.DataFrame(pca.fit_transform(embeddings))\n",
    "    #     tmp_data = pd.merge(data,embeddings, how='left', left_index=True, right_index=True, sort=True, copy=True,\n",
    "    #                         indicator=False)\n",
    "    #     tmp_data = tmp_data.drop(['tokens'], axis=1)\n",
    "    #     tmp_data = tmp_data.dropna()\n",
    "    #     tmp_data = tmp_data.reset_index().iloc[:, 1:]\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_post2_csv(sql):\n",
    "    data = get_sql_data(sql)\n",
    "    print('data read in' + str(len(data)))\n",
    "    posts = all_posts_oneday(data)\n",
    "    print('all_posts_oneday finish' + str(len(posts)))\n",
    "    posts2 = merge_days_posts(5, posts)\n",
    "    print('merge_posts finish' + str(len(posts2)))\n",
    "    posts2.to_csv('posts2.csv')\n",
    "    print('get posts2 csv file')\n",
    "\n",
    "\n",
    "def get_w2v_csv():\n",
    "    reader = pd.read_csv('posts2.csv', chunksize=10000)\n",
    "    loop = True\n",
    "    n = 1\n",
    "    for chunk in reader:\n",
    "        chunk = chunk.iloc[:, 1:]\n",
    "        chunk['created_time'] = pd.to_datetime(chunk['created_time'], format=\"%Y-%m-%d\")\n",
    "        token_chunk = tokenize(chunk)\n",
    "        print(len(token_chunk))\n",
    "        dataset_chunk = merge_posts_price(token_chunk, closeprice, 0)\n",
    "        print(len(dataset_chunk))\n",
    "        final_chunk = word_to_vec(dataset_chunk)\n",
    "        print(len(final_chunk))\n",
    "        if n == 1:\n",
    "            # print(len(final_chunk))\n",
    "            final_chunk.to_csv('final_dataset.csv', mode='w')\n",
    "            print(n)\n",
    "            n += 1\n",
    "        else:\n",
    "            final_chunk.to_csv('final_dataset.csv', mode='a', header=None)\n",
    "            print(n)\n",
    "            n += 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    path_to_model = 'googlew2v.bin'\n",
    "    output_file = 'googlew2v.txt'\n",
    "    bin2txt(path_to_model, output_file)\n",
    "\n",
    "\n",
    "def bin2txt(path_to_model, output_file):\n",
    "    output = codecs.open(output_file, 'w', 'utf-8')\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "    print('Done loading Word2Vec!')\n",
    "    vocab = model.vocab\n",
    "    for item in vocab:\n",
    "        vector = list()\n",
    "        for dimension in model[item]:\n",
    "            vector.append(str(dimension))\n",
    "        vector_str = \",\".join(vector)\n",
    "        line = item + \"\\t\" + vector_str\n",
    "        output.writelines(line + \"\\n\")  # 本来用的是write（）方法，但是结果出来换行效果不对。改成writelines（）方法后还没试过。\n",
    "    output.close()\n",
    "\n",
    "\n",
    "def draw_3d(model):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('googlew2v.bin', binary=True)\n",
    "    print('pre-trained bin read in ')\n",
    "    vocab = model.vocab\n",
    "    with open('finance_words.txt', 'r', encoding='utf-8') as f:\n",
    "        words = f.read()\n",
    "    sample = words.split('\\n')\n",
    "    sample1 = []\n",
    "    dic = list(vocab.keys())\n",
    "    for i in range(0, len(sample)):\n",
    "        if sample[i] in dic:\n",
    "            sample1.append(sample[i])\n",
    "    sample = sample1[:50]\n",
    "    df_sample = pd.DataFrame(model[sample])\n",
    "    df_sample.insert(0, 'word', sample)\n",
    "    pca = PCA(n_components=3, random_state=0)\n",
    "    pcs = pd.DataFrame(pca.fit_transform(df_sample.iloc[:, 1:]))\n",
    "    pcs.insert(0, 'word', df_sample['word'])\n",
    "    return pcs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sql = 'select * from preprocessed_text_test'\n",
    "    word2vec_path = \"C:/Users/guoti/Desktop/group/googlew2v.bin\"\n",
    "    word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "    print('w2c bin file read in')\n",
    "    closeprice = get_close_price_nday(3)\n",
    "    print('cp file read in')\n",
    "    get_post2_csv(sql)\n",
    "    get_w2v_csv()\n",
    "    pcs = draw_3d(word2vec)\n",
    "\n",
    "    print('start to draw')\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.axes(projection='3d')\n",
    "    z = pcs.iloc[:, 1]\n",
    "    x = pcs.iloc[:, 2]\n",
    "    y = pcs.iloc[:, 3]\n",
    "    ax1.scatter3D(x, y, z)\n",
    "    for i in range(0, len(x)):\n",
    "        ax1.text(pcs.iloc[i, 1], pcs.iloc[i, 2], pcs.iloc[i, 3], pcs['word'][i])\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
